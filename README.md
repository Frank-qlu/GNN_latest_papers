<p align="center">
 <img width="200px" src="https://github.com/Frank-qlu/GNN_latest_papers/blob/main/imgs/title.png" align="center" alt="GNN latest papers" />
 <h2 align="center">GNN latest papers</h2>
<p align="center" >**The latest research papers on graph representation learning**</p>
</p>

 **Key Keywords**: Graph representation Learning, graph pooling, graph transformer

# Contents

- [Graph representation Learning](#Graph representation Learning)
  - [Basic Models](#Basic Models)
  - [graph pooling](#graph pooling)
  - [graph transformer](#graph transformer)
  - [graph self-supervised learning](#graph self-supervised learning)
- [GNN Applications](#GNN Applications)
  - [Chemical Molecules](#Chemical Molecules)
# Graph representation Learning

## Basic Models
- [ICLR 2022] Graph-Less Neural Networks: Teaching Old MLPs New Tricks Via Distillation [[Paper](https://arxiv.org/abs/2110.08727)] [[Code](https://github.com/snap-research/graphless-neural-networks)]
- [ICLR 2022] LSPE: Graph Neural Networks with Learnable Structural and Positional Representations [[Paper](https://arxiv.org/abs/2110.07875)] [[Code](https://github.com/vijaydwivedi75/gnn-lspe)]
- [ICLR 2022] TOGL: Topological Graph Neural Networks [[Paper](https://arxiv.org/abs/2102.07835v4)] [[Code](https://github.com/borgwardtlab/togl)]
- [ICLR 2022] GOAT: Graph Ordering Attention Networks [[Paper](https://arxiv.org/pdf/2204.05351.pdf)] [[Code](https://github.com/MichailChatzianastasis/GOAT)] 

## graph pooling

- [CIKM 2021] Pooling Architecture Search for Graph Classification [[Paper](https://arxiv.org/abs/2108.10587)] [[Code](https://github.com/AutoML-Research/PAS)] 
- [ICLR 2021] GMTPool: Accurate Learning of Graph Representations with Graph Multiset Pooling [[Paper](https://openreview.net/pdf?id=JHcqXGaqiGn)] [[Code](https://github.com/JinheonBaek/GMT)] 
- [NIPS 2021] GraphTrans: Representing Long-Range Context for Graph Neural Networks with Global Attention [[Paper](https://proceedings.neurips.cc/paper/2021/file/6e67691b60ed3e4a55935261314dd534-Paper.pdf)] [[Code](https://github.com/ucbrise/graphtrans)] 
- [TKDE 2021] MVPool [[Paper](https://ieeexplore.ieee.org/abstract/document/9460814)] [[Code](https://github.com/cszhangzhen/MVPool)] 
- [TPAMI 2021] TAPool: Topology-Aware Graph Pooling Networks [[Paper](https://paperswithcode.com/paper/topology-aware-graph-pooling-networks)]
- [SIGIR 2021] CGIPool: Graph Pooling via Coarsened Graph Infomax [[Paper](https://arxiv.org/pdf/2105.01275.pdf)] [[Code](https://github.com/PangYunsheng8/CGIPool)] 
- [TNNLS 2021] Ipool: Information-Based Pooling in Hierarchical Graph Neural Networks [[Paper](https://ieeexplore.ieee.org/document/9392315)]
- [ICLR 2020] StructPool: Structured Graph Pooling via Conditional Random Fields  [[Paper](https://openreview.net/forum?id=BJxg_hVtwH)] [[Code](https://github.com/Nate1874/StructPool)]
- [WWW 2020] GSAPool: Structure-Feature based Graph Self-adaptive Pooling [[Paper](https://arxiv.org/pdf/2002.00848.pdf)] [[Code](https://github.com/psp3dcg/GSAPool)]
- [NIPS 2020] VIPool: Graph Cross Networks with Vertex Infomax Pooling [[Paper](https://papers.nips.cc/paper/2020/file/a26398dca6f47b49876cbaffbc9954f9-Paper.pdf)] [[Code](https://github.com/limaosen0/GXN)] 
- [AAAI 2020] ASAP: Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations [[Paper](https://arxiv.org/abs/1911.07979)] [[Code](https://github.com/malllabiisc/ASAP)] 
- [ICML 2020] HaarPool: Graph Pooling with Compressive Haar Basis [[Paper](https://arxiv.org/abs/1909.11580v2)] [[Code](https://github.com/YuGuangWang/HaarPool)] 
- [ICML 2020] minCUTPool:Spectral Clustering with Graph Neural Networks for Graph Pooling [[Paper](https://arxiv.org/pdf/1907.00481.pdf)] 
- [AAAI 2020] HGP-SL: Hierarchical Graph Pooling with Structure Learning [[Paper](https://arxiv.org/abs/1911.05954)] [[Code](https://github.com/cszhangzhen/HGP-SL)] 
- [ICML 2019] SAGPool: Self-Attention Graph Pooling [[Paper](https://arxiv.org/abs/1904.08082)] [[Code](https://github.com/inyeoplee77/SAGPool)]
- [arXiv 2019] EdgePool: Edge Contraction Pooling for Graph Neural Networks [[Paper](https://arxiv.org/abs/1905.10990)] [[Code](https://github.com/rusty1s/pytorch_geometric/tree/master/benchmark/kernel)]
- [ICML 2019] gpool: Graph U-Nets [[Paper](http://proceedings.mlr.press/v97/gao19a/gao19a.pdf)] [[Code](https://github.com/HongyangGao/Graph-U-Nets)] 
- [NIPS 2018] DiffPool: Hierarchical Graph Representation Learning with Differentiable Pooling [[Paper](https://paperswithcode.com/paper/hierarchical-graph-representation-learning)] [[Code](https://paperswithcode.com/paper/hierarchical-graph-representation-learning)] 
- [AAAI 2018] SortPool: An End-to-End Deep Learning Architecture for Graph Classification [[Paper](https://muhanzhang.github.io/papers/AAAI_2018_DGCNN.pdf)] [[Code](https://github.com/muhanzhang/pytorch_DGCNN)]
- [ICLR 2016] Set2set: Order Matters: Sequence to Sequence for Sets [[Paper](https://arxiv.org/abs/1511.06391)] [[Code](https://paperswithcode.com/paper/order-matters-sequence-to-sequence-for-sets)] 
- [NIPS 2016] DCNN: Diffusion-Convolutional Neural Networks [[Paper](https://proceedings.neurips.cc/paper/2016/file/390e982518a50e280d8e2b535462ec1f-Paper.pdf)] [[Code](https://paperswithcode.com/paper/diffusion-convolutional-neural-networks)] 

## graph transformer
- [arXiv 2022] Pure Transformers are Powerful Graph Learners [[Paper](https://arxiv.org/pdf/2207.02505.pdf)] [[Code](https://github.com/jw9730/tokengt)]
- [NIPS 2021] GraphGPS: Recipe for a General, Powerful, Scalable Graph Transformer [[Paper](https://arxiv.org/pdf/2205.12454.pdf)] [[Code](https://github.com/rampasek/GraphGPS)]
- [ICML 2022] SAT: Structure-Aware Transformer for Graph Representation Learning [[Paper](https://arxiv.org/abs/2202.03036)] [[Code](https://github.com/borgwardtlab/sat)] 
- [NIPS 2021] Graphormer: Do Transformers Really Perform Bad for Graph Representationï¼Ÿ [[Paper](https://arxiv.org/abs/2106.05234)] [[Code](https://github.com/Microsoft/Graphormer)] 
- [NIPS 2021] GraphTrans: Representing Long-Range Context for Graph Neural Networks with Global Attention [[Paper](https://proceedings.neurips.cc/paper/2021/file/6e67691b60ed3e4a55935261314dd534-Paper.pdf)] [[Code](https://github.com/ucbrise/graphtrans)]
- [NIPS 2021] SAN: Rethinking Graph Transformers with Spectral Attention [[Paper](https://arxiv.org/pdf/2106.03893v3.pdf)] [[Code](https://github.com/DevinKreuzer/SAN)] 
- [arXiv 2021] GraphiT: Encoding Graph Structure in Transformers [[Paper](https://arxiv.org/abs/2106.05667)] [[Code](https://github.com/inria-thoth/GraphiT)]
- [arXiv 2020] Graph-Bert: Only Attention is Needed for Learning Graph Representations [[Paper](https://arxiv.org/pdf/2001.05140.pdf)] [[Code](https://github.com/jwzhanggy/Graph-Bert)] 
- [NIPS 2020] GROVER: Self-Supervised Graph Transformer on Large-Scale Molecular Data [[Paper](https://proceedings.neurips.cc/paper/2020/file/94aef38441efa3380a3bed3faf1f9d5d-Paper.pdf)] [[Code](https://github.com/tencent-ailab/grover)]
- [AAAI 2020 Workshop] graphtransformer: A Generalization of Transformer Networks to Graphs [[Paper](https://arxiv.org/pdf/2012.09699.pdf)] [[Code](https://github.com/graphdeeplearning/graphtransformer)] 
- [arXiv 2021] GraphiT: Encoding Graph Structure in Transformers [[Paper](https://arxiv.org/abs/2106.05667)] [[Code](https://github.com/inria-thoth/GraphiT)]
- [NIPS 2022] NodeFormer: A Scalable Graph Structure Learning Transformer for Node Classification[[Paper](https://openreview.net/pdf?id=sMezXGG5Soopenreview.net/pdf?id=sMezXGG5So)] [[Code](https://github.com/qitianwu/NodeFormergithub.com/qitianwu/NodeFormer)]
## graph self-supervised learning
- [KDD 2022] GraphMAE: Self-Supervised Masked Graph Autoencoders [[Paper](https://arxiv.org/abs/2205.10803)] [[Code](https://github.com/thudm/graphmae)]


# GNN Applications

## Chemical Molecules
- [ICLR 2022] Molecular Contrastive Learning with Chemical Element Knowledge Graph [[Paper](https://arxiv.org/abs/2112.00544)] [[Code](https://github.com/ZJU-Fangyin/KCL)]

:star:<p>The latest GNN research papers are continuously updated. If you have some questions,please email: lizhipengqilu@gmail. Besides, If you like this project, please fork or star</p>
